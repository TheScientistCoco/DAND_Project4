{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Report\n",
    "\n",
    "Yu Tao 2020/12/26\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "This is a brief report on the twitter data wrangling steps: gather, assess, and clean. The Twitter data used is from **WeRateDogs**, a Twitter account that rates people's dogs with a humorous comment about the dog.\n",
    "\n",
    "### Data Gathering:\n",
    "\n",
    "Three datasets have been gathered from various sources for this project.\n",
    "\n",
    "**The WeRateDogs Twitter archive**:\n",
    "\n",
    "This archive contains basic tweet data (tweet ID, timestamp, text, etc.) for all 5000+ of the tweets from WeRateDogs as they stood on August 1, 2017. This file is directly downloadable, with the name **twitter_archive_enhanced.csv**.\n",
    "\n",
    "**The tweet image predictions**: \n",
    "\n",
    "This file shows what breed of dog (or other object, animal, etc.) is present in each tweet according to a neural network. This file (**image_predictions.tsv**) is hosted on Udacity's servers and is downloaded programmatically using the Requests library and a link URL.\n",
    "\n",
    "**Additional Twitter data**:\n",
    "\n",
    "Each tweet's favorite count and retweet count are important for analysis. I created a Twitter API account and used the tweet IDs in the WeRateDogs Twitter archive, queried the Twitter API for each tweet's JSON data using Python's **Tweepy** library and store each tweet's entire set of JSON data in a file called **tweet_json.txt** file. From this JSON file, I extracted the tweet ID, retweet count, and favorite count, and utilized them in the final analysis.\n",
    "\n",
    "### Data Assessing:\n",
    "\n",
    "For each of the datasets, I import them into separate pandas dataframes, and assessed their properties based on the **quality and tidiness** of the data. This is because, for data downloaded from various sources, it might contain issues such as missing values, outliers, etc. There might also be wrong or inaccurate values when we programmatically extract information. \n",
    "\n",
    "I used basic pandas commands (**head, info, value_count, sample, groupby, duplicate**) to assess the data from different aspects. After exploration, I found **11 quality issues and 2 tidiness issues** for the three dataframes.\n",
    "\n",
    "**df_twitter_archive (quality):**\n",
    "\n",
    "- there are missing values in the dataframe.\n",
    "- there are retweets and replies in the dataframe.\n",
    "- the data type of timestamp should be change to datetime.\n",
    "- some ratings are not correctly extracted from the text (float number issue).\n",
    "- sometimes there are more than one ratings for single tweet.\n",
    "- some of the dog names are wrong (with names like 'a', 'the', 'an').\n",
    "- many of the dogs don't have a dog type (doggo/floofer/pupper/puppo).\n",
    "\n",
    "**df_image_prediction (quality):**\n",
    "\n",
    "- there are duplicate predictions from retweets.\n",
    "- some pictures got predicted even though they don't show dogs.\n",
    "- the prediction results are inconsistent in using lower/upper case letters.\n",
    "- we should condense the 3 predictions into one based on the confidence level.\n",
    "\n",
    "**df_twitter_archive (tidiness):**\n",
    "\n",
    "- the four columns (doggo/floofer/pupper/puppo) should be combined into one.\n",
    "\n",
    "**all 3 dataframes (tidiness)**:\n",
    "\n",
    "- we should merge the three tables since they share tweet_id.\n",
    "\n",
    "### Data Cleaning:\n",
    "\n",
    "Data cleaning is divided into three parts: **define, code, and test**. For each of the issues listed in the assessing section, I used commands like drop, merge, count to fix them one by one.\n",
    "\n",
    "I have also learned other skills from the wrangling process, such as text processing when I dealt with rating numerator and demoninator, and string operations when combine columns. \n",
    "\n",
    "Besides, it is a good practice to make copies of the original dataframe, because whenever you make a mistake cleaning you data, you can easilty track back using the copies data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
